{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04bf1e8-6ce6-4b62-8eef-5a464c221bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/study/nlp_project/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a436bc1d-87d4-4904-81e6-0a96c6efcf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Text: INTERNSHIP REPORT ON \n",
      "Digital e sales dashboard \n",
      "By Sayan Sahu \n",
      "VT2024 \n",
      "Under the guidance of Mr.Rahul Kumar \n",
      "About the Project \n",
      "Introduction \n",
      "The project aims to develop a comprehensive digital sales dashboard \n",
      "using Tableau to visualize and analyze sales data, enabling \n",
      "stakeholders to make data-driven decisions.\n",
      "SCOPE\n",
      "The scope of the project includes data collection, data preprocessing, \n",
      "dashboard design, and implementation using Tableau. The dashboard \n",
      "provides insights into key sales metri\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_text = read_pdf('resumes/sample_resume.pdf')  # Ensure the file name matches your actual file\n",
    "print(\"PDF Text:\", pdf_text[:500])  # Print first 500 characters for preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0895552-3779-4615-bae8-ed64dd07c32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: internship report on digital e sales dashboard by sayan sahu vt2024 under the guidance of mr.rahul kumar about the project introduction the project aims to develop a comprehensive digital sales dashboard using tableau to visualize and analyze sales data, enabling stakeholders to make data-driven decisions. scope the scope of the project includes data collection, data preprocessing, dashboard design, and implementation using tableau. the dashboard provides insights into key sales metrics, trends,\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Preprocess the PDF text\n",
    "processed_text = preprocess_text(pdf_text)\n",
    "print(\"Processed Text:\", processed_text[:500])  # Print first 500 characters for preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b96f51-7910-41c1-af5e-dccf2c64c06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Entities: [('sayan sahu vt2024', 'PERSON'), ('mr.rahul kumar', 'PERSON'), ('1', 'CARDINAL'), ('monthly', 'DATE'), ('the past two years', 'DATE'), ('monthly', 'DATE'), ('months and years', 'DATE'), ('2', 'CARDINAL'), ('a month', 'DATE'), ('3', 'CARDINAL'), ('first', 'ORDINAL'), ('second', 'ORDINAL'), ('5', 'CARDINAL'), ('5', 'CARDINAL'), ('5', 'CARDINAL'), ('4', 'CARDINAL'), ('first', 'ORDINAL'), ('first', 'ORDINAL'), ('months and year', 'DATE'), ('second', 'ORDINAL'), ('month', 'DATE'), ('year', 'DATE'), ('5', 'CARDINAL'), ('south east asia', 'LOC'), ('6', 'CARDINAL'), ('north asia', 'LOC'), ('year and months', 'DATE'), ('7', 'CARDINAL'), ('red & green threshold', 'ORG'), ('8', 'CARDINAL'), ('9', 'CARDINAL'), ('3', 'CARDINAL'), ('4', 'CARDINAL'), ('5', 'CARDINAL'), ('months and years', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Extract entities from the processed text\n",
    "entities = extract_entities(processed_text)\n",
    "print(\"Extracted Entities:\", entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f4274a-3cb3-48be-8006-84ec2bf70280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches with Criteria: {'skills': [], 'locations': ['south east asia', 'north asia'], 'experience_years': ['sayan sahu vt2024', '2', '3', '5', '5', '5', '5', '3', '5']}\n"
     ]
    }
   ],
   "source": [
    "# Define criteria for shortlisting\n",
    "criteria = {\n",
    "    'skills': ['tableau', 'data analysis'],\n",
    "    'locations': ['south east asia', 'north asia'],\n",
    "    'experience_years': ['2', '3', '5']\n",
    "}\n",
    "\n",
    "def check_criteria(entities, criteria):\n",
    "    matches = {key: [] for key in criteria}\n",
    "    \n",
    "    for entity, label in entities:\n",
    "        for key, values in criteria.items():\n",
    "            if any(value.lower() in entity.lower() for value in values):\n",
    "                matches[key].append(entity)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Check if the extracted entities match the defined criteria\n",
    "matches = check_criteria(entities, criteria)\n",
    "print(\"Matches with Criteria:\", matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69d1ba8-0ab7-43a8-bd99-9c71770b69f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Matches with Criteria: {'skills': ['tableau'], 'locations': ['south east asia', 'north asia'], 'experience_years': ['2', '3', '5']}\n"
     ]
    }
   ],
   "source": [
    "def check_criteria_advanced(text, criteria):\n",
    "    matches = {key: [] for key in criteria}\n",
    "    \n",
    "    text = text.lower()  # Convert entire text to lowercase for case-insensitive matching\n",
    "    \n",
    "    for key, values in criteria.items():\n",
    "        for value in values:\n",
    "            if value.lower() in text:\n",
    "                matches[key].append(value)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Check if the full text matches the defined criteria\n",
    "advanced_matches = check_criteria_advanced(processed_text, criteria)\n",
    "print(\"Advanced Matches with Criteria:\", advanced_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e024ed-ca99-4aed-9f3c-7cb37a8c689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching resumes: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of matching resumes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_matches))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Save the results to a file\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43msave_results\u001b[49m(all_matches, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlp_project/matching_results.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlp_project/matching_results.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_results' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "\n",
    "# Define function to read all PDF files in a directory\n",
    "def read_all_pdfs(directory):\n",
    "    pdf_texts = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            pdf_texts[file_name] = read_pdf(file_path)\n",
    "    return pdf_texts\n",
    "\n",
    "# Process all resumes in the 'resumes' folder\n",
    "pdf_texts = read_all_pdfs('resumes')\n",
    "\n",
    "# Check which resumes match the criteria\n",
    "def process_all_resumes(pdf_texts, criteria):\n",
    "    results = {}\n",
    "    for file_name, text in pdf_texts.items():\n",
    "        processed_text = preprocess_text(text)\n",
    "        matches = check_criteria_advanced(processed_text, criteria)\n",
    "        if any(matches.values()):  # Check if there are any matches\n",
    "            results[file_name] = matches\n",
    "    return results\n",
    "\n",
    "# Get the results for all resumes\n",
    "all_matches = process_all_resumes(pdf_texts, criteria)\n",
    "print(\"Number of matching resumes:\", len(all_matches))\n",
    "\n",
    "# Save the results to a file\n",
    "save_results(all_matches, 'nlp_project/matching_results.txt')\n",
    "print(\"Results saved to 'nlp_project/matching_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c4bbc-7642-4128-877d-cd830e076f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for file_name, match_details in results.items():\n",
    "            f.write(f\"Resume: {file_name}\\n\")\n",
    "            for category, items in match_details.items():\n",
    "                f.write(f\"  {category.capitalize()}:\\n\")\n",
    "                for item in items:\n",
    "                    f.write(f\"   - {item}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Save the results to a file again\n",
    "save_results(all_matches, 'nlp_project/matching_results.txt')\n",
    "print(\"Results saved to 'nlp_project/matching_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ce1a4-46ef-4eb8-b642-3ae38b911737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for file_name, match_details in results.items():\n",
    "            f.write(f\"Resume: {file_name}\\n\")\n",
    "            for category, items in match_details.items():\n",
    "                f.write(f\"  {category.capitalize()}:\\n\")\n",
    "                for item in items:\n",
    "                    f.write(f\"   - {item}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Save the results to a file again\n",
    "save_results(all_matches, 'nlp_project/matching_results.txt')\n",
    "print(\"Results saved to 'nlp_project/matching_results.txt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b749f14-86e3-4e2a-9c57-4ac429438412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "\n",
    "# Define function to read all PDF files in a directory\n",
    "def read_all_pdfs(directory):\n",
    "    pdf_texts = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            pdf_texts[file_name] = read_pdf(file_path)\n",
    "    return pdf_texts\n",
    "\n",
    "# Process all resumes in the 'resumes' folder\n",
    "pdf_texts = read_all_pdfs('resumes')\n",
    "\n",
    "# Check which resumes match the criteria\n",
    "def process_all_resumes(pdf_texts, criteria):\n",
    "    results = {}\n",
    "    for file_name, text in pdf_texts.items():\n",
    "        processed_text = preprocess_text(text)\n",
    "        matches = check_criteria_advanced(processed_text, criteria)\n",
    "        if any(matches.values()):  # Check if there are any matches\n",
    "            results[file_name] = matches\n",
    "    return results\n",
    "\n",
    "# Get the results for all resumes\n",
    "all_matches = process_all_resumes(pdf_texts, criteria)\n",
    "print(\"Number of matching resumes:\", len(all_matches))\n",
    "\n",
    "# Save the results to a file\n",
    "save_results(all_matches, 'nlp_project/matching_results.txt')\n",
    "print(\"Results saved to 'nlp_project/matching_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab73fcf-758a-4894-9f69-12f0fee4f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List contents of the current working directory\n",
    "print(\"Contents of the current directory:\")\n",
    "print(os.listdir('.'))\n",
    "\n",
    "# List contents of the 'nlp_project' directory to ensure it exists\n",
    "print(\"\\nContents of 'nlp_project' directory:\")\n",
    "print(os.listdir('nlp_project'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6e3d4-cf22-4bb7-899b-d6b9353bb09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Re-create the directory\n",
    "os.makedirs('nlp_project', exist_ok=True)\n",
    "\n",
    "# Define the path for the results file\n",
    "results_file_path = 'nlp_project/matching_results.txt'\n",
    "\n",
    "# Save the results to the file\n",
    "def save_results(results, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for file_name, match_details in results.items():\n",
    "            f.write(f\"Resume: {file_name}\\n\")\n",
    "            for category, items in match_details.items():\n",
    "                f.write(f\"  {category.capitalize()}:\\n\")\n",
    "                for item in items:\n",
    "                    f.write(f\"   - {item}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "save_results(all_matches, results_file_path)\n",
    "print(f\"Results saved to '{results_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796f894-9530-46b7-b25e-3c2263773e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def process_resume(file_path):\n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def match_criteria(entities):\n",
    "    # Define your criteria here\n",
    "    criteria = {\n",
    "        'skills': ['tableau'],\n",
    "        'locations': ['south east asia', 'north asia'],\n",
    "        'experience_years': ['2', '3', '5']\n",
    "    }\n",
    "    \n",
    "    matches = {'skills': [], 'locations': [], 'experience_years': []}\n",
    "    \n",
    "    # Check for skills\n",
    "    for skill in criteria['skills']:\n",
    "        if skill.lower() in ' '.join([e[0].lower() for e in entities]):\n",
    "            matches['skills'].append(skill)\n",
    "    \n",
    "    # Check for locations\n",
    "    for loc in criteria['locations']:\n",
    "        if loc.lower() in ' '.join([e[0].lower() for e in entities]):\n",
    "            matches['locations'].append(loc)\n",
    "    \n",
    "    # Check for experience years\n",
    "    for exp in criteria['experience_years']:\n",
    "        if exp in [e[0] for e in entities]:\n",
    "            matches['experience_years'].append(exp)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Process all resumes in the directory\n",
    "def process_all_resumes(directory):\n",
    "    all_matches = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            entities = process_resume(file_path)\n",
    "            match_details = match_criteria(entities)\n",
    "            if any(match_details.values()):  # Only add if there are matches\n",
    "                all_matches[file_name] = match_details\n",
    "    return all_matches\n",
    "\n",
    "# Directory containing resumes\n",
    "resumes_directory = 'resumes'\n",
    "\n",
    "# Process resumes and save results\n",
    "all_matches = process_all_resumes(resumes_directory)\n",
    "save_results(all_matches, 'nlp_project/matching_results.txt')\n",
    "print(f\"Results saved to 'nlp_project/matching_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe65d2-5112-4c29-a4c4-e0e99de59b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the criteria for matching resumes\n",
    "criteria = {\n",
    "    'skills': ['python', 'tableau', 'sql'],  # Example skills\n",
    "    'locations': ['south east asia', 'north asia'],  # Example locations\n",
    "    'experience_years': ['2', '3', '5']  # Example experience years\n",
    "}\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {docx_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {'skills': [], 'locations': [], 'experience_years': []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG', 'LOC', 'DATE', 'CARDINAL']:\n",
    "            if ent.label_ == 'ORG':\n",
    "                entities['skills'].append(ent.text.lower())\n",
    "            elif ent.label_ == 'LOC':\n",
    "                entities['locations'].append(ent.text.lower())\n",
    "            elif ent.label_ in ['DATE', 'CARDINAL']:\n",
    "                entities['experience_years'].append(ent.text.lower())\n",
    "    return entities\n",
    "\n",
    "def matches_criteria(entities, criteria):\n",
    "    matches = {\n",
    "        'skills': [skill for skill in entities['skills'] if skill in criteria['skills']],\n",
    "        'locations': [loc for loc in entities['locations'] if loc in criteria['locations']],\n",
    "        'experience_years': [exp for exp in entities['experience_years'] if exp in criteria['experience_years']]\n",
    "    }\n",
    "    return matches\n",
    "\n",
    "def process_resumes(folder_path):\n",
    "    all_matches = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.lower().endswith('.pdf'):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        elif file_name.lower().endswith('.docx'):\n",
    "            text = extract_text_from_docx(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        entities = extract_entities(text)\n",
    "        matches = matches_criteria(entities, criteria)\n",
    "        if any(matches.values()):  # Only include if there's any match\n",
    "            all_matches[file_name] = matches\n",
    "    return all_matches\n",
    "\n",
    "# Set the path to your resumes directory\n",
    "resumes_directory = 'resumes'\n",
    "all_matches = process_resumes(resumes_directory)\n",
    "\n",
    "# Save the results to a file\n",
    "def save_results(results, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for file_name, match_details in results.items():\n",
    "            f.write(f\"Resume: {file_name}\\n\")\n",
    "            for category, matches in match_details.items():\n",
    "                f.write(f\"{category.capitalize()}: {', '.join(matches)}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "save_results(all_matches, 'nlp_project/matching_results.txt')\n",
    "print(\"Results saved to 'nlp_project/matching_results.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff454e-9e1b-4856-b597-4d9d65499152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != 'en':\n",
    "        translated = translator(text, src_lang=lang, tgt_lang='en')\n",
    "        return translated[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Your resume text here\"\n",
    "processed_text = detect_and_translate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdae2b-c0de-40cf-8b12-1fa302e56899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != 'en':\n",
    "        translated = translator(text, src_lang=lang, tgt_lang='en')\n",
    "        return translated[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Your resume text here\"\n",
    "processed_text = detect_and_translate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded1e62-2f63-4f57-8575-cabf81f4abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != 'en':\n",
    "        translated = translator(text, src_lang=lang, tgt_lang='en')\n",
    "        return translated[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Your resume text here\"\n",
    "processed_text = detect_and_translate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace074b7-a814-4406-a149-b7a65b052236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != 'en':\n",
    "        translated = translator(text, src_lang=lang, tgt_lang='en')\n",
    "        return translated[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Your resume text here\"\n",
    "processed_text = detect_and_translate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc725acb-62ec-48d6-ba7c-1c9f0eb26698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e85181-1d79-4340-a4ef-1ed4418f3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != 'en':\n",
    "        translated = translator(text, src_lang=lang, tgt_lang='en')\n",
    "        return translated[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Your resume text here\"\n",
    "processed_text = detect_and_translate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddf32b-d53f-4309-803f-3a2822f7ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != 'en':\n",
    "        translated = translator(text, src_lang=lang, tgt_lang='en')\n",
    "        return translated[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Your resume text here\"\n",
    "processed_text = detect_and_translate(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fcf34-a8c1-4901-9be0-15b85486672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722a748-abf5-4fa5-8f72-71525cb94065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57327e-d8ed-40f8-a808-b7fd65f453e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ef446-6c92-442d-a82f-0ca8d9e5c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Check if PyTorch is being used\n",
    "print(\"Is PyTorch available?\", torch.cuda.is_available())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0926b-4000-492c-92ef-69ead66860e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Is PyTorch available?\", torch.cuda.is_available())\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c8977-a84f-4c13-9fa4-39209878457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize translation pipeline with PyTorch explicitly\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\", framework=\"pt\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != 'en':\n",
    "        translated = translator(text, src_lang=lang, tgt_lang='en')\n",
    "        return translated[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Your resume text here\"\n",
    "processed_text = detect_and_translate(text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847d8dd-9da7-4c0a-87d3-9f2aa0e3c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -c \"import torch; print(torch.__version__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186b57d-6995-4354-88fc-5bd192cee114",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -c \"import torch; print(torch.cuda.is_available())\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f239c-94b7-4141-acfa-e7868dce6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)  # Ensure you have the `detect` function implemented\n",
    "    if lang != 'en':\n",
    "        translated = translator(text, src_lang=lang, tgt_lang='en')\n",
    "        return translated[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Your resume text here\"\n",
    "processed_text = detect_and_translate(text)\n",
    "print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10d6eb-c2a7-4538-a085-a24cefc28abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Check if PyTorch is available\n",
    "import torch\n",
    "print(\"Is PyTorch available?\", torch.cuda.is_available())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    print(f\"Detected language: {lang}\")\n",
    "    if lang != 'en':\n",
    "        translation = translator(text, src_lang=lang, tgt_lang='en')[0]['translation_text']\n",
    "        return translation\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Bonjour, comment ça va?\"\n",
    "translated_text = detect_and_translate(sample_text)\n",
    "print(f\"Translated text: {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f938b-454a-4cb5-925a-96a782604003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from urllib3.exceptions import NotOpenSSLWarning\n",
    "warnings.simplefilter(\"ignore\", NotOpenSSLWarning)\n",
    "\n",
    "from langdetect import detect\n",
    "from transformers import pipeline\n",
    "\n",
    "# Check if PyTorch is available\n",
    "import torch\n",
    "print(\"Is PyTorch available?\", torch.cuda.is_available())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Import tf-keras\n",
    "import tf_keras as keras\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    print(f\"Detected language: {lang}\")\n",
    "    if lang != 'en':\n",
    "        translation = translator(text, src_lang=lang, tgt_lang='en')[0]['translation_text']\n",
    "        return translation\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Bonjour, comment ça va?\"\n",
    "translated_text = detect_and_translate(sample_text)\n",
    "print(f\"Translated text: {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2411e57d-1283-4266-91cf-e6d6ec3f26ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is PyTorch available? False\n",
      "PyTorch version: 2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: fr\n",
      "Translated text: Hello, how's it going?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from langdetect import detect\n",
    "\n",
    "print(\"Is PyTorch available?\", torch.cuda.is_available())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    print(\"Detected language:\", lang)\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        print(\"Translated text:\", translated_text)\n",
    "    else:?\n",
    "        print(\"Text is already in English:\", text)\n",
    "\n",
    "# Test the function with a sample text\n",
    "sample_text = \"Bonjour, comment ça va?\"\n",
    "detect_and_translate(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12685566-5872-48d0-986a-e085535e2bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c02131d-e373-4423-82ba-4fcb4333383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test output!\n"
     ]
    }
   ],
   "source": [
    "print(\"This is a test output!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cacf066d-0f05-4020-aa8f-4f0c1ef217c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Text is already in English: INTERNSHIP REPORT ON \n",
      "Digital e sales dashboard \n",
      "By Sayan Sahu \n",
      "VT2024 \n",
      "Under the guidance of Mr.Rahul Kumar \n",
      "About the Project \n",
      "Introduction \n",
      "The project aims to develop a comprehensive digital sales dashboard \n",
      "using Tableau to visualize and analyze sales data, enabling \n",
      "stakeholders to make data-driven decisions.\n",
      "SCOPE\n",
      "The scope of the project includes data collection, data preprocessing, \n",
      "dashboard design, and implementation using Tableau. The dashboard \n",
      "provides insights into key sales metrics, trends, and performance \n",
      "indicators.\n",
      "Tools and Technologies: \n",
      "-Tableau Desktop:For data visualization and dashboard creation \n",
      "-MS Excel:For data collection and preprocessing. \n",
      "-Data sheet:Sales data from the company's database. \n",
      "PROBLEM STATEMENT \n",
      "Problem Statement 1. \n",
      "The CFO of an electronics chain is interested in gaining a better \n",
      "understanding of sales and \n",
      "profits. She has a very specific question of the data \n",
      "\"What do monthly sales and profits look like over the past two years?\" \n",
      " \n",
      "This is what  monthly sales and profit looks like past 2 years we also \n",
      "have a filter on the right hand side of the screen for getting more \n",
      "specific data distribution we can select months and years according to \n",
      "our need. \n",
      "Problem Statement 2. \n",
      "The CFO liked this but now feels that the picture is a bit misleading \n",
      "since we are looking at \n",
      "absolute numbers for profit. \n",
      "\"Can you fix the previous viz to show profit margin instead of the \n",
      "absolute profit numbers?” \n",
      "Solution: \n",
      "To calculate Profit margin the formula we have used is \n",
      "SUM([Profit]) / SUM([Sales]) \n",
      "We have created a new calculated parameter and defined this formula \n",
      "in it. \n",
      " \n",
      " \n",
      "This is our Profit Margin being shown at each bar for sales and profit \n",
      "values.Here too we have a Month and Year filter on our right side \n",
      "panel for viewing according to our need. \n",
      "Problem Statement 3. \n",
      "Management has a new request: \"Can you show us, on the same \n",
      "chart, both profits and sales by \n",
      "product sub-category in decending order of sales?” \n",
      "Bonus: Can you think of a way to focus management on the key \n",
      "product categories and not all of the many \n",
      "small items that the company sells? \n",
      "On focussing on the first part of our problem we need to have Product \n",
      "sub category and Year in our rows and profit and sales value in our \n",
      "column our sheet will look like this . \n",
      "For the second part of our problem we need to add a top N filter where \n",
      "N will be 5 so that it can show only top 5 key selling products on the \n",
      "basis of profit margin. \n",
      "This is what our filter looks like and given below is our sheet \n",
      " \n",
      "  \n",
      "This is what our sheet looks like showing the top 5 company products \n",
      "depending upon the profit margin alongside profit and sales. \n",
      "Problem Statement 4. \n",
      "The regional sales managers of Superstore are interested in an analysis \n",
      "of sales and profit by \n",
      "product category, sub-category and region. They will use this \n",
      "information to discuss growth opportunities \n",
      "for new products and possible pricing changes or product cancellation \n",
      "ideas. \n",
      "They want to know: \"What do our sales and profits look like by \n",
      "product category, sub-category and \n",
      "region? Also, can you give us the ability to drill from category to \n",
      "sub-category that was \n",
      "purchased?\" \n",
      "Bonus: Can i look at the trend for each segment within the same view \n",
      "Lets first focus on the first part of our problem where the product \n",
      "category ,sub category and region is in column and sales and profit in \n",
      "rows. \n",
      "This is how it looks like having months and year filter on the right \n",
      "hand side. \n",
      "For second part of our problem we need to have a look at trend of \n",
      "each segment within the same view for this we can add month to the \n",
      "rows of the existing sheet. \n",
      "Here we again have a filter on the right hand side for filtering on basis \n",
      "of month and year \n",
      "Problem Statement 5. \n",
      "The Sales Manager for South East Asia doesn't want to see a viz of \n",
      "profit and sales by product \n",
      "category and sub-category. \n",
      "\"The bar chart is awesome, but can you show me a cross-tab of \n",
      "the data? I need to see actual \n",
      "values!” \n",
      "For this we need to have Product category and sub category on rows \n",
      "and sales and profit in columns and we’ll select the graph type to gantt \n",
      "bar \n",
      "Problem Statement 6. \n",
      "After the Regional Sales Managers reviewed your analysis the RM for \n",
      "North Asia called and asked \n",
      "for more details. \n",
      "His question: \"I like the crosstab, but I also need to quickly see \n",
      "where my best and worst \n",
      "performers are. Can you build me a view highlighting sales by \n",
      "profits?” \n",
      "For viewing best and worst performers we need to have a new \n",
      "calculated field as sales to profit ratio where the formula we have use \n",
      "d is this \n",
      "This is include in the column and give it to color property and the \n",
      "sheet we obtain is given below \n",
      "Here our sheet is very interactive with higher STP ratio in red colours \n",
      "showing the best performers .We also have a filter on the right side \n",
      "panel for filtering on basis of year and months. \n",
      "Problem Statement 7. \n",
      "“I like the highlight table, but I need to define my own values for \n",
      "what is Red and Green. Can \n",
      "you help me some traffic lighting?” \n",
      "To enable the HR to give his/her own colour values we have to create \n",
      "a new parameters Green threshold and Red Threshold and then create \n",
      "a new calculated field named as color field implementing Red & \n",
      "Green Threshold which one can input.  \n",
      " \n",
      "This is what our sheet would look like here in Red Threshold one can \n",
      "input the values and in Green also determining at what values the red \n",
      "and green color appears. \n",
      "Problem Statement 8. \n",
      "Now things were getting exciting, the managers scheduled a follow-up \n",
      "meeting with you and asked for \n",
      "additional analysis. \n",
      "\"Can you create a geographic map to distribute to the state \n",
      "managers that shows profits and sales down \n",
      "to the city level? We also need the ability to choose the Product \n",
      "Category that is displayed.” \n",
      "This is what our map looks like showing the state with more sales \n",
      "with deeper color and on hovering over each state you can easily see \n",
      "the profit value the sales value and the product subcategory too. \n",
      "Problem Statement 9. \n",
      " Create a Sales Dashboard using all the questionnaire shared \n",
      "above. \n",
      "Problem statement 1 &2 dashboard. \n",
      " \n",
      "Problem statement 3 dashboard. \n",
      "Problem statement 4 dashboard. \n",
      "Problem statement 5 & 6 dashboard. \n",
      "You can select months and years according to your need it is very \n",
      "interactive. \n",
      "Problem statement 7 & 8 dashboard. \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    print(\"Detected language:\", lang)\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        print(\"Translated text:\", translated_text)\n",
    "    else:\n",
    "        print(\"Text is already in English:\", text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Extract text from the resume PDF\n",
    "pdf_path = \"resumes/sample_resume.pdf\"\n",
    "resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Process the extracted text\n",
    "detect_and_translate(resume_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f27a50dd-2b1b-4058-ae2b-38885f2779b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65f984c5-820f-4b66-9d06-18c3d48e1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e88cd00-cbc2-4cde-9950-73adafe4a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        return translated_text\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eebbb43f-7720-4d2f-82df-fd580bd10edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meets_criteria(text, criteria):\n",
    "    return any(keyword in text.lower() for keyword in criteria)\n",
    "\n",
    "def shortlist_resumes(resumes_folder, criteria):\n",
    "    shortlisted_resumes = []\n",
    "    for filename in os.listdir(resumes_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(resumes_folder, filename)\n",
    "            resume_text = extract_text_from_pdf(pdf_path)\n",
    "            translated_text = detect_and_translate(resume_text)\n",
    "            if meets_criteria(translated_text, criteria):\n",
    "                shortlisted_resumes.append(filename)\n",
    "    return shortlisted_resumes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8892adef-228b-4bbc-b3d1-df727fdc0fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlisted Resumes: []\n"
     ]
    }
   ],
   "source": [
    "resumes_folder = \"resumes\"\n",
    "criteria = [\"data analysis\", \"machine learning\"]\n",
    "shortlisted_files = shortlist_resumes(resumes_folder, criteria)\n",
    "print(\"Shortlisted Resumes:\", shortlisted_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58aeed65-b072-40e2-9010-37f0d72512ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlisted Resumes: ['sample_resume.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "# Define function to read PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Define function to detect language and translate text if necessary\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "# Define function to process resumes and apply criteria\n",
    "def process_resumes(resumes_folder, criteria):\n",
    "    resume_texts = []\n",
    "    resume_files = [f for f in os.listdir(resumes_folder) if f.endswith('.pdf')]\n",
    "    \n",
    "    for resume_file in resume_files:\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "        translated_text = detect_and_translate(text)\n",
    "        resume_texts.append(translated_text)\n",
    "\n",
    "    # Apply criteria\n",
    "    vectorizer = CountVectorizer().fit_transform(resume_texts)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Example of filtering resumes based on criteria\n",
    "    # Here you would implement your specific criteria for filtering resumes\n",
    "    # For simplicity, this example assumes you want to select the first 5 resumes\n",
    "    filtered_indices = cosine_matrix[0].argsort()[-5:][::-1]\n",
    "    filtered_resumes = [resume_files[i] for i in filtered_indices]\n",
    "    \n",
    "    return filtered_resumes\n",
    "\n",
    "# Define criteria for shortlisting (this should be customized based on your needs)\n",
    "criteria = {}  # Example criteria dictionary, modify as needed\n",
    "\n",
    "# Run the process\n",
    "resumes_folder = 'resumes'\n",
    "shortlisted_resumes = process_resumes(resumes_folder, criteria)\n",
    "print(\"Shortlisted Resumes:\", shortlisted_resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f7966-7918-4644-b60a-4d43348c5ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c15901a-cde4-48db-b520-5467f9160d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlisted Resumes: ['sample_resume.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "# Define function to read PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Define function to detect language and translate text if necessary\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "# Define function to process resumes and apply criteria\n",
    "def process_resumes(resumes_folder, criteria):\n",
    "    resume_texts = []\n",
    "    resume_files = [f for f in os.listdir(resumes_folder) if f.endswith('.pdf')]\n",
    "    \n",
    "    for resume_file in resume_files:\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "        translated_text = detect_and_translate(text)\n",
    "        resume_texts.append(translated_text)\n",
    "\n",
    "    # Apply criteria\n",
    "    vectorizer = CountVectorizer().fit_transform(resume_texts)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Example of filtering resumes based on criteria\n",
    "    # Here you would implement your specific criteria for filtering resumes\n",
    "    # For simplicity, this example assumes you want to select the first 5 resumes\n",
    "    filtered_indices = cosine_matrix[0].argsort()[-5:][::-1]\n",
    "    filtered_resumes = [resume_files[i] for i in filtered_indices]\n",
    "    \n",
    "    return filtered_resumes\n",
    "\n",
    "# Define criteria for shortlisting (this should be customized based on your needs)\n",
    "criteria = {}  # Example criteria dictionary, modify as needed\n",
    "\n",
    "# Run the process\n",
    "resumes_folder = 'resumes'\n",
    "shortlisted_resumes = process_resumes(resumes_folder, criteria)\n",
    "print(\"Shortlisted Resumes:\", shortlisted_resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ea18b4-4dc5-4db4-9d76-37841b7e12ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlisted Resumes: ['sample_resume.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "# Initialize translation pipeline with device argument for GPU usage\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\", device=device)\n",
    "\n",
    "# Define function to read PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Define function to detect language and translate text if necessary\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "# Define function to process resumes and apply criteria\n",
    "def process_resumes(resumes_folder, criteria):\n",
    "    resume_texts = []\n",
    "    resume_files = [f for f in os.listdir(resumes_folder) if f.endswith('.pdf')]\n",
    "    \n",
    "    for resume_file in resume_files:\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "        translated_text = detect_and_translate(text)\n",
    "        resume_texts.append(translated_text)\n",
    "\n",
    "    # Apply criteria\n",
    "    vectorizer = CountVectorizer().fit_transform(resume_texts)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Example of filtering resumes based on criteria\n",
    "    # Here you would implement your specific criteria for filtering resumes\n",
    "    # For simplicity, this example assumes you want to select the first 5 resumes\n",
    "    filtered_indices = cosine_matrix[0].argsort()[-5:][::-1]\n",
    "    filtered_resumes = [resume_files[i] for i in filtered_indices]\n",
    "    \n",
    "    return filtered_resumes\n",
    "\n",
    "# Define criteria for shortlisting (this should be customized based on your needs)\n",
    "criteria = {}  # Example criteria dictionary, modify as needed\n",
    "\n",
    "# Run the process\n",
    "resumes_folder = 'resumes'\n",
    "shortlisted_resumes = process_resumes(resumes_folder, criteria)\n",
    "print(\"Shortlisted Resumes:\", shortlisted_resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "373e7705-a38b-4903-a70c-a4eb14ac0c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Shortlisted Resumes: ['sample_resume.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "# Define function to read PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Define function to detect language and translate text if necessary\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    print(f\"Detected language: {lang}\")  # Debug: Print detected language\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        print(f\"Translated text: {translated_text}\")  # Debug: Print translated text\n",
    "        return translated_text\n",
    "    return text\n",
    "\n",
    "# Define function to process resumes and apply criteria\n",
    "def process_resumes(resumes_folder, criteria):\n",
    "    resume_texts = []\n",
    "    resume_files = [f for f in os.listdir(resumes_folder) if f.endswith('.pdf')]\n",
    "    \n",
    "    for resume_file in resume_files:\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "        translated_text = detect_and_translate(text)\n",
    "        resume_texts.append(translated_text)\n",
    "\n",
    "    # Apply criteria\n",
    "    vectorizer = CountVectorizer().fit_transform(resume_texts)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Example of filtering resumes based on criteria\n",
    "    # Here you would implement your specific criteria for filtering resumes\n",
    "    # For simplicity, this example assumes you want to select the first 5 resumes\n",
    "    filtered_indices = cosine_matrix[0].argsort()[-5:][::-1]\n",
    "    filtered_resumes = [resume_files[i] for i in filtered_indices]\n",
    "    \n",
    "    return filtered_resumes\n",
    "\n",
    "# Define criteria for shortlisting (this should be customized based on your needs)\n",
    "criteria = {}  # Example criteria dictionary, modify as needed\n",
    "\n",
    "# Run the process\n",
    "resumes_folder = 'resumes'\n",
    "shortlisted_resumes = process_resumes(resumes_folder, criteria)\n",
    "print(\"Shortlisted Resumes:\", shortlisted_resumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23d441e3-ab19-4bd8-834f-479aecd639ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlisted Resumes saved to 'shortlisted_resumes.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "# Define function to read PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Define function to detect language and translate text if necessary\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        return translated_text, lang\n",
    "    return text, lang\n",
    "\n",
    "# Define function to process resumes and apply criteria\n",
    "def process_resumes(resumes_folder, criteria):\n",
    "    resume_texts = []\n",
    "    resume_files = [f for f in os.listdir(resumes_folder) if f.endswith('.pdf')]\n",
    "    resume_info = []\n",
    "\n",
    "    for resume_file in resume_files:\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "        translated_text, lang = detect_and_translate(text)\n",
    "        resume_texts.append(translated_text)\n",
    "        resume_info.append((resume_file, lang))\n",
    "\n",
    "    # Apply criteria\n",
    "    vectorizer = CountVectorizer().fit_transform(resume_texts)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Example of filtering resumes based on criteria\n",
    "    filtered_indices = cosine_matrix[0].argsort()[-5:][::-1]\n",
    "    filtered_resumes = [resume_info[i] for i in filtered_indices]\n",
    "    \n",
    "    return filtered_resumes\n",
    "\n",
    "# Define criteria for shortlisting (this should be customized based on your needs)\n",
    "criteria = {}  # Example criteria dictionary, modify as needed\n",
    "\n",
    "# Run the process\n",
    "resumes_folder = 'resumes'\n",
    "shortlisted_resumes = process_resumes(resumes_folder, criteria)\n",
    "\n",
    "# Save results to a text file\n",
    "with open(\"shortlisted_resumes.txt\", \"w\") as file:\n",
    "    for resume, lang in shortlisted_resumes:\n",
    "        file.write(f\"{resume}: Detected language - {lang}\\n\")\n",
    "\n",
    "print(\"Shortlisted Resumes saved to 'shortlisted_resumes.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78217a21-d0fa-4a53-b37f-314ba87c663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Shortlisted Resumes saved to 'shortlisted_resumes.docx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from docx import Document\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "# Define function to read PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Define function to detect language and translate text if necessary\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    print(f\"Detected language: {lang}\")  # Debug: Print detected language\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        print(f\"Translated text: {translated_text}\")  # Debug: Print translated text\n",
    "        return translated_text, lang\n",
    "    return text, lang\n",
    "\n",
    "# Define function to process resumes and apply criteria\n",
    "def process_resumes(resumes_folder, criteria):\n",
    "    resume_texts = []\n",
    "    resume_files = [f for f in os.listdir(resumes_folder) if f.endswith('.pdf')]\n",
    "    resume_info = []\n",
    "\n",
    "    for resume_file in resume_files:\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "        translated_text, lang = detect_and_translate(text)\n",
    "        resume_texts.append(translated_text)\n",
    "        resume_info.append((resume_file, lang, translated_text))\n",
    "\n",
    "    # Apply criteria\n",
    "    vectorizer = CountVectorizer().fit_transform(resume_texts)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Example of filtering resumes based on criteria\n",
    "    filtered_indices = cosine_matrix[0].argsort()[-5:][::-1]\n",
    "    filtered_resumes = [resume_info[i] for i in filtered_indices]\n",
    "    \n",
    "    return filtered_resumes\n",
    "\n",
    "# Define criteria for shortlisting (this should be customized based on your needs)\n",
    "criteria = {}  # Example criteria dictionary, modify as needed\n",
    "\n",
    "# Run the process\n",
    "resumes_folder = 'resumes'\n",
    "shortlisted_resumes = process_resumes(resumes_folder, criteria)\n",
    "\n",
    "# Save results to a Word document\n",
    "doc = Document()\n",
    "doc.add_heading('Shortlisted Resumes', 0)\n",
    "\n",
    "for resume, lang, text in shortlisted_resumes:\n",
    "    doc.add_paragraph(f\"Resume: {resume}\")\n",
    "    doc.add_paragraph(f\"Detected language: {lang}\")\n",
    "    doc.add_paragraph(f\"Translated text: {text}\")\n",
    "    doc.add_paragraph()  # Add a blank line between entries\n",
    "\n",
    "doc.save(\"shortlisted_resumes.docx\")\n",
    "\n",
    "print(\"Shortlisted Resumes saved to 'shortlisted_resumes.docx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76a0551e-d0b6-4b56-8b45-532cb11438d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Shortlisted Resumes saved to 'shortlisted_resumes.docx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from transformers import pipeline\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from docx import Document\n",
    "\n",
    "# Initialize translation pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "\n",
    "# Define function to read PDF and extract text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    pdf_document.close()\n",
    "    return text\n",
    "\n",
    "# Define function to detect language and translate text if necessary\n",
    "def detect_and_translate(text):\n",
    "    lang = detect(text)\n",
    "    print(f\"Detected language: {lang}\")  # Debug: Print detected language\n",
    "    if lang != \"en\":\n",
    "        translated_text = translator(text)[0]['translation_text']\n",
    "        print(f\"Translated text: {translated_text}\")  # Debug: Print translated text\n",
    "        return translated_text, lang\n",
    "    return text, lang\n",
    "\n",
    "# Define function to clean up text\n",
    "def clean_text(text):\n",
    "    # Replace special characters and extra spaces\n",
    "    text = text.replace(' ",
    "', '\\n')  # Replace special line breaks\n",
    "    text = text.replace('\\n\\n', '\\n')  # Remove extra line breaks\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    return text\n",
    "\n",
    "# Define function to process resumes and apply criteria\n",
    "def process_resumes(resumes_folder, criteria):\n",
    "    resume_texts = []\n",
    "    resume_files = [f for f in os.listdir(resumes_folder) if f.endswith('.pdf')]\n",
    "    resume_info = []\n",
    "\n",
    "    for resume_file in resume_files:\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "        translated_text, lang = detect_and_translate(text)\n",
    "        cleaned_text = clean_text(translated_text)\n",
    "        resume_texts.append(cleaned_text)\n",
    "        resume_info.append((resume_file, lang, cleaned_text))\n",
    "\n",
    "    # Apply criteria\n",
    "    vectorizer = CountVectorizer().fit_transform(resume_texts)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    # Example of filtering resumes based on criteria\n",
    "    filtered_indices = cosine_matrix[0].argsort()[-5:][::-1]\n",
    "    filtered_resumes = [resume_info[i] for i in filtered_indices]\n",
    "    \n",
    "    return filtered_resumes\n",
    "\n",
    "# Define criteria for shortlisting (this should be customized based on your needs)\n",
    "criteria = {}  # Example criteria dictionary, modify as needed\n",
    "\n",
    "# Run the process\n",
    "resumes_folder = 'resumes'\n",
    "shortlisted_resumes = process_resumes(resumes_folder, criteria)\n",
    "\n",
    "# Save results to a Word document\n",
    "doc = Document()\n",
    "doc.add_heading('Shortlisted Resumes', 0)\n",
    "\n",
    "for resume, lang, text in shortlisted_resumes:\n",
    "    doc.add_paragraph(f\"Resume: {resume}\")\n",
    "    doc.add_paragraph(f\"Detected language: {lang}\")\n",
    "    doc.add_paragraph(\"Translated text:\")\n",
    "    doc.add_paragraph(text)\n",
    "    doc.add_paragraph()  # Add a blank line between entries\n",
    "\n",
    "doc.save(\"shortlisted_resumes.docx\")\n",
    "\n",
    "print(\"Shortlisted Resumes saved to 'shortlisted_resumes.docx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0323445-3cd2-4d5e-96e2-d46e15d723a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
